{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download images from the World Steel Association website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages to scrap data from web\n",
    "import requests #package for send requests to web and download html contents\n",
    "from bs4 import BeautifulSoup #package for parsing html contents\n",
    "from tqdm import tqdm #package for progress bar\n",
    "\n",
    "#ssl warining off\n",
    "requests.packages.urllib3.disable_warnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://worldsteel.org/about-steel/lovesteel/\" #url to scrap\n",
    "res = requests.get(url, verify=False) #send request to web server and get response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.text, \"html.parser\") #parse html contents\n",
    "soup #print parsed html contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select(\".fact-box > img\") #check image elements(=target to scrap) with css selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save images to local directory using for loop\n",
    "for image in soup.select(\".fact-box > img\"):\n",
    "    img_url = image[\"data-src\"] #get image url\n",
    "    img_name = img_url.split(\"/\")[-1] #get image name\n",
    "    img_data = requests.get(img_url, verify=False) #get image data\n",
    "    open(img_name, \"wb\").write(img_data.content) #save file to write binary image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping HRM(Human Resource Management) glossary data from SHRM website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#set url to scrap : SHRM HR Glossary page\n",
    "url_for_list = \"https://www.shrm.org/ResourcesAndTools/tools-and-samples/HR-Glossary/_layouts/15/SHRM.Core/ajax/AutomatedViewViaSPS.aspx\"\n",
    "\n",
    "#set parameters to scrap\n",
    "params = {\n",
    "    \"ItemUniqueId\": \"4b37a8f9-2314-477f-ae7a-956782aeef9b\",\n",
    "    \"Page\": \"0\",\n",
    "    \"PageSize\": \"200\",\n",
    "    \"PageView\": \"List\",\n",
    "    \"AdSection\": \"SectionFrontAutomatedBox\",\n",
    "    \"AdSectionMobile\": \"SectionFrontAutomatedBoxMobile\",\n",
    "    \"AdSize1W\": \"300\",\n",
    "    \"AdSize1H\": \"250\",\n",
    "    \"AdsCount\": \"1\",\n",
    "    \"TimeStamp\": \"638348396995574667\",\n",
    "    \"Random\": \"0.5497595531528323\"\n",
    "}\n",
    "\n",
    "#send request to web server and get response\n",
    "res = requests.get(url_for_list, params=params, verify=False)\n",
    "\n",
    "#parse html contents\n",
    "soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "#get glossay word elements list with css selector\n",
    "word_list = soup.select(\"#pan_Items > div\")\n",
    "\n",
    "#check length of word list\n",
    "print(\"total cnt : {}\".format(len(word_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign empty list to save glossary data\n",
    "glossary_list = []\n",
    "\n",
    "#to stop loop, assign 0 to cnt\n",
    "cnt = 0\n",
    "\n",
    "#loop for word list\n",
    "for word in tqdm(word_list):\n",
    "    #if each word has no link, skip\n",
    "    if len(word.select(\"a\")) == 0:\n",
    "        continue\n",
    "    word_name = word.select(\"a\")[0].text #get word name\n",
    "    word_link = word.select(\"a\")[0][\"href\"] #get word link\n",
    "    if \"https\" in word_link: #if word link is absolute url, try to get word description\n",
    "        res = requests.get(word_link, verify=False) #send request to web server and get response\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\") #parse html contents\n",
    "        desc = soup.select(\".shrm-Element-P\") #find and get word description\n",
    "        if len(desc) == 0: #if word description is empty, skip\n",
    "            continue\n",
    "        word_desc = soup.select(\".shrm-Element-P\")[0].text.strip() #get word description\n",
    "\n",
    "        #append word name, link, description to glossary list\n",
    "        glossary_list.append({ \"name\" : word_name, \"link\" : word_link, \"desc\" : word_desc })\n",
    "\n",
    "    #increase cnt\n",
    "    cnt += 1\n",
    "\n",
    "    #if cnt is 30, break loop -> to scrap only 30 words for test\n",
    "    #if you want to scrap all words, remove below if statement\n",
    "    if cnt == 30:\n",
    "        break\n",
    "    \n",
    "#convert glossary list to dataframe\n",
    "df = pd.DataFrame(glossary_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dataframe to excel file\n",
    "df.to_excel(\"glossary.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
